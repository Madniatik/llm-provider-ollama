{
    "$schema": "https://bithoven.dev/schemas/llm-config-v1.json",
    "version": "0.1.0",
    "metadata": {
        "package": "bithoven/llm-provider-ollama",
        "created_at": "2025-12-12T04:00:00Z",
        "updated_at": "2025-12-12T04:00:00Z",
        "author": "Bithoven Team"
    },
    "configuration": {
        "name": "Llama 3.2 3B (Fast & Lightweight)",
        "slug": "llama-3-2-3b",
        "provider": "ollama",
        "model_name": "llama3.2:3b",
        "description": "Compact Llama model - ideal for fast responses and resource-constrained environments",
        "api_endpoint": "http://localhost:11434/api/chat",
        "default_parameters": {
            "max_tokens": 2048,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "num_predict": 2048,
            "repeat_penalty": 1.1,
            "seed": null,
            "stop": null
        },
        "capabilities": [
            "text-generation",
            "instruction-following",
            "chat",
            "summarization",
            "streaming"
        ],
        "limits": {
            "context_window": 131072,
            "max_output_tokens": 2048,
            "model_size_gb": 2,
            "min_ram_gb": 4,
            "recommended_ram_gb": 8
        },
        "pricing": {
            "currency": "XXX",
            "input_per_1k_tokens": 0.0,
            "output_per_1k_tokens": 0.0,
            "note": "Self-hosted, no API costs"
        },
        "recommended_use_cases": [
            "Quick chat responses",
            "Simple text generation",
            "Summarization",
            "Basic Q&A",
            "Prototyping"
        ],
        "tags": ["recommended", "lightweight", "fast", "cost-effective", "meta", "llama"],
        "is_active": true,
        "is_default": true,
        "hardware_requirements": {
            "gpu": "Optional (CPU-only capable)",
            "cpu": "Any modern processor",
            "disk_space": "2GB"
        }
    }
}
